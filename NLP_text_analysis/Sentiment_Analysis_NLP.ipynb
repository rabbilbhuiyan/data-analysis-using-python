{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of text as postive or negative using sci-kit learn machine learning model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlines\n",
    "1. Data preparation \n",
    "2. Model selection\n",
    "3. Analysis and evaluation\n",
    "4. Improveing the model\n",
    "5. Saving the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set\n",
    "Source: amazon.com, the amazon review as training data\n",
    "full data source: http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "Open the data (Books_small.json) in sublime text and explore the data.\n",
    "- The data is a json file and each line is another json object\n",
    "- We can see the 'reviewText' in the data which is text content in the review\n",
    "- another one is 'overall' in the data which is out of 5 star rating\n",
    "- As these two field is important so we will take care of these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating enum class/regular class of programming (optional)\n",
    "class Sentiment: #call the class Sentiment\n",
    "    NEGATIVE ='NEGATIVE' # properties of the class\n",
    "    NEUTRAL ='NEUTRAL'\n",
    "    POSITIVE = 'POSITIVE'\n",
    "    \n",
    "\n",
    "# Making class review (it' much neater and helps keep track of things)\n",
    "class Review: # call the class Review\n",
    "    def __init__(self, text, score): # initialize with self (always) and our data fields\n",
    "        self.text = text\n",
    "        self.score = score\n",
    "        self.sentiment = self.get_sentiment()\n",
    "    \n",
    "    #creating function withing review for getting setiment\n",
    "    def get_sentiment(self): # all functions within a class pass in itself\n",
    "        if self.score <= 2:\n",
    "            #return 'NEGATIVE' \n",
    "            # This also works- we used Sentiment class just to be consistent with the string (so that not to float around/accidently take wrong thing)\n",
    "            return Sentiment.NEGATIVE # refering this to sentiment class we created on top\n",
    "        elif self.score == 3:\n",
    "            #return 'NEUTRAL'\n",
    "            return Sentiment.NEUTRAL\n",
    "        else: # score of 4 or 5\n",
    "            #return 'POSITIVE'\n",
    "            return Sentiment.POSITIVE\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"reviewerID\": \"A1E5ZR1Z4OQJG\", \"asin\": \"1495329321\", \"reviewerName\": \"Pure Jonel \\\"Pure Jonel\\\"\", \"helpful\": [0, 0], \"reviewText\": \"Da Silva takes the divine by storm with this unique new novel.  She develops a world unlike any others while keeping it firmly in the real world.  This is a very well written and entertaining novel.  I was quite impressed and intrigued by the way that this solid storyline was developed, bringing the readers right into the world of the story.  I was engaged throughout and definitely enjoyed my time spent reading it.I loved the character development in this novel.  Da Silva creates a cast of high school students who actually act like high school students.  I really appreciated the fact that none of them were thrown into situations far beyond their years, nor did they deal with events as if they had decades of life experience under their belts.  It was very refreshing and added to the realism and impact of the novel.  The friendships between the characters in this novel were also truly touching.Overall, this novel was fantastic.  I can&#8217;t wait to read more and to find out what happens next in the series.  I&#8217;d definitely recommend this debut novel by Da Silva to those who want a little YA fun with a completely unique & shocking storyline.Please note that I received a complimentary copy of this work in exchange for an honest review.\", \"overall\": 4.0, \"summary\": \"An amazing first novel\", \"unixReviewTime\": 1396137600, \"reviewTime\": \"03 30, 2014\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# impor json library\n",
    "import json\n",
    "# loading data line by line\n",
    "# First create file name\n",
    "file_name = './sentiment/Books_small.json'\n",
    "\n",
    "# open up the file with open\n",
    "with open (file_name) as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        break # here we get the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Da Silva takes the divine by storm with this unique new novel.  She develops a world unlike any others while keeping it firmly in the real world.  This is a very well written and entertaining novel.  I was quite impressed and intrigued by the way that this solid storyline was developed, bringing the readers right into the world of the story.  I was engaged throughout and definitely enjoyed my time spent reading it.I loved the character development in this novel.  Da Silva creates a cast of high school students who actually act like high school students.  I really appreciated the fact that none of them were thrown into situations far beyond their years, nor did they deal with events as if they had decades of life experience under their belts.  It was very refreshing and added to the realism and impact of the novel.  The friendships between the characters in this novel were also truly touching.Overall, this novel was fantastic.  I can&#8217;t wait to read more and to find out what happens next in the series.  I&#8217;d definitely recommend this debut novel by Da Silva to those who want a little YA fun with a completely unique & shocking storyline.Please note that I received a complimentary copy of this work in exchange for an honest review.\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# now it wannna get the reviewText\n",
    "with open (file_name) as f:\n",
    "    for line in f:\n",
    "        review =json.loads(line) # use json library to load line as review like dictionary\n",
    "        print(review['reviewText'])\n",
    "        print(review['overall'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, we got all the things we need (two important fields out of json file)\n",
    "- Now we gater all this in a nice way, we will get rid of this above code/statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Love the book, great story line, keeps you entertained.for a first novel from this author she did a great job,  Would definitely recommend!',\n",
       " 4.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file in more organized way (indexing of reviewText and score)\n",
    "reviews = [] # create reviews empty list\n",
    "with open (file_name) as f:\n",
    "        for line in f:\n",
    "            review =json.loads(line) # use json library to load line as review like dictionary\n",
    "            reviews.append((review['reviewText'],review['overall'])) # append the reviews list and tuple objects of reviewText and a score\n",
    "        # we used tuple here as we have two field/argument, becuase append takes only one argument, by using tuple we put two arguments\n",
    "# check it it works (just put review number)\n",
    "reviews[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing to get reviewText and score (it works)\n",
    "#reviews[5][0] # use 0 index to access text\n",
    "reviews[5][1] # use 1 index to access score/overall rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- This is not yet the neatiest one\n",
    "- As a data scientist, at some point the data get messy and it gets hard\n",
    "  to parse through someone's code\n",
    "- So, we gonna do real quick by making a data class for all data we loaded in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Review at 0x22b0cbc99c8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now instead of appending tuple, create a Review object and pass in the text and the score \n",
    "reviews = [] # create reviews empty list\n",
    "with open (file_name) as f:\n",
    "    for line in f:\n",
    "        review =json.loads(line) # use json library to load line as review like dictionary\n",
    "        reviews.append(Review(review['reviewText'],review['overall'])) # instead of review, we used Review class that we created on top\n",
    "         \n",
    "# check it it works (just put review number)\n",
    "reviews[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reviews[5].text\n",
    "reviews[5].score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, it is much neatier and helps keep tracking the things easily\n",
    "- Now, we will initialize sentiment in our class, for adding sentiment class\n",
    "- we need to create a function as above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[5].sentiment # works both way - without class sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[5].sentiment # with class sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing /preparation\n",
    "- Machine learning model does not love text data/ hard to read text data that we have\n",
    "- Rather ML model loves matrices and numerical data as input (numerical vector)\n",
    "- So, we need to convert the text data into quantative vector\n",
    "- For that we will start with bag-of-words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many reviews we have in our data\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splittting the data (1000 review) into train and test data set \n",
    "from sklearn.model_selection import train_test_split\n",
    "# as we pass single review list, so we get two times output as training(or x)and test (or y)\n",
    "training, test = train_test_split(reviews, test_size = 0.33, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670, 330)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cheking the split data\n",
    "len(training), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will fit the bag-of-words model into the trainig set\n",
    "- then we will build a classifer on that training set\n",
    "- and we will test everyting on our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivid characters and descriptions. The author has created a tale that grabs your attention and I couldn't put it down.\n",
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# printing the first text\n",
    "\n",
    "print(training[0].text)\n",
    "print(training[0].sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Steps are:\n",
    "- we will pass the data into bag-of-words vectorizer\n",
    "- broadly, we will take the text and predict whether it is positive or negative\n",
    "- so, the x would the text in our model and y the category/sentiment\n",
    "- now split the training data into training x and train y and also test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training data\n",
    "train_x = [x.text for x in training] # to get text for x, apply list comprehension\n",
    "train_y = [x.sentiment for x in training]\n",
    "\n",
    "# splitting test data\n",
    "test_x = [x.text for x in test] # for test our model we pass in test x\n",
    "test_y = [x.sentiment for x in test] # to see if it matches up closley with test y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vivid characters and descriptions. The author has created a tale that grabs your attention and I couldn't put it down.\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking first text for train_x\n",
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking first setiment for train_y\n",
    "train_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<670x7372 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 41455 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# our corpus is train_x\n",
    "vectorizer.fit_transform(train_x) # fit_transform method\n",
    "\n",
    "# two steps of code: but perform same function as above (fit and tranform method)\n",
    "#vectorizer.fit(x_train)\n",
    "#train_x_vectors = vectorizer.transform(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get here sparse matrix with 670 training vlaues (documents), all of them \n",
    "have their own row and each one of those rows have 7372 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x7372 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_vectors = vectorizer.fit_transform(train_x) # apply both fit and transform to train_x\n",
    "\n",
    "# numerical presentation of our test set text and we just do tranform here, as we don't fit a new model here\n",
    "test_x_vectors = vectorizer.transform(test_x) # apply transform only to test_x\n",
    "\n",
    "\n",
    "train_x_vectors[0] # print a matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vivid characters and descriptions. The author has created a tale that grabs your attention and I couldn't put it down.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0] # print just piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivid characters and descriptions. The author has created a tale that grabs your attention and I couldn't put it down.\n",
      "  (0, 7086)\t1\n",
      "  (0, 1148)\t1\n",
      "  (0, 350)\t2\n",
      "  (0, 1800)\t1\n",
      "  (0, 6595)\t1\n",
      "  (0, 562)\t1\n",
      "  (0, 3054)\t1\n",
      "  (0, 1558)\t1\n",
      "  (0, 6475)\t1\n",
      "  (0, 6593)\t1\n",
      "  (0, 2895)\t1\n",
      "  (0, 7353)\t1\n",
      "  (0, 539)\t1\n",
      "  (0, 1515)\t1\n",
      "  (0, 5197)\t1\n",
      "  (0, 3545)\t1\n",
      "  (0, 2007)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(train_x_vectors[0]) # print a matrix that represemt the text so it counts all position that are nonzero in our matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivid characters and descriptions. The author has created a tale that grabs your attention and I couldn't put it down.\n",
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# let's look the matrix in more tradional way\n",
    "print(train_x[0])\n",
    "print(train_x_vectors[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so our final data for building the model is\n",
    " - train_x_vectors and train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final data for building the model (we will fit a model around these two)\n",
    "#train_x_vectors\n",
    "#train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choosing the right model\n",
    "- we have no x and y, so let's do some classficiation \n",
    "- There are many models, let's take coiple of models and check how they perform\n",
    "- or part of test any model: train model, run it on test data seeing what performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "clf_svm.fit(train_x_vectors, train_y) # pass in x and y to fit this classifier to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x7372 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 36 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now predict something (based on training SVM classifier)\n",
    "test_x[0] # it looks like positive review\n",
    "# let's get vector for that\n",
    "test_x_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's predict wheter this text test_X[0] is positive or negative\n",
    "clf_svm.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_x_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_dec.predict(test_x_vectors[0]) # This also predicts positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(train_x_vectors.toarray(), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gnb.predict(test_x_vectors.toarray())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_log = DecisionTreeClassifier()\n",
    "clf_log.fit(train_x_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_log.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the classifiers predicted psoitive for the first test set, so look pretty goods initially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8242424242424242\n",
      "0.7787878787878788\n",
      "0.8121212121212121\n",
      "0.7393939393939394\n"
     ]
    }
   ],
   "source": [
    "# let's look entire test set and see how accurately we predict every one of those test examples\n",
    "# Mean Accuracy on all test labels (how many lables are actually predicted correctly)\n",
    "print(clf_svm.score(test_x_vectors, test_y)) # comapre prediction of test_x_vector to our actual testy_y\n",
    "print(clf_dec.score(test_x_vectors, test_y))\n",
    "print(clf_gnb.score(test_x_vectors.toarray(), test_y))\n",
    "print(clf_log.score(test_x_vectors, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91319444, 0.22222222, 0.21052632])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More important metric (recommended for data scientist)\n",
    "# F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(test_y,clf_svm.predict(test_x_vectors), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE, Sentiment.NEUTRAL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see, f1 score as 91% postitive, but it is pretty trash for netural \n",
    "and negative labels, so we can improve the model further (we look for the model that equally predict for all categories). Let's check \n",
    "for the other model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87804878, 0.06896552, 0.14035088])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y,clf_dec.predict(test_x_vectors), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE, Sentiment.NEUTRAL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89678511, 0.09090909, 0.08510638])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y,clf_gnb.predict(test_x_vectors.toarray()), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE, Sentiment.NEUTRAL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85612789, 0.        , 0.0952381 ])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y,clf_log.predict(test_x_vectors), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE, Sentiment.NEUTRAL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POSITIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How could we imporve the model acuracy\n",
    "# As all the classfier predicted well for positive, so there is no issue with modeling\n",
    "# So, let's look at the data, if we can  improve the model\n",
    "\n",
    "train_y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting how many actual positive labels we have\n",
    "train_y.count(Sentiment.POSITIVE)\n",
    "# we had 670 train lables in total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So right away, our models are gonna be heavily biased towards these positive lables as there's way more of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting how many actual negative labels we have\n",
    "train_y.count(Sentiment.NEGATIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rest of the review would be neutral\n",
    "# counting how many actual neturallabels we have\n",
    "train_y.count(Sentiment.NEUTRAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we got our issue for bad f1 score, we need to balace our negative and postive data.\n",
    "The way to do this is adding more data. Let's work with 10000 files !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model with a library called pickle\n",
    "# we need to save the model so that we can use it later and avoid retraining the model\n",
    "import pickle\n",
    "\n",
    "with open('./sentiment/sentiment_classifer_1000_samples.pkl', 'wb') as f:\n",
    "    pickle.dump(clf_svm, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
