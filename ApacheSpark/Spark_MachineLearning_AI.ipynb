{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "emp_df = spark.read.csv('D:\\LinkedIn_Learning\\ApcheSpark\\employee.txt', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, last_name: string, email: string, gender: string, department: string, start_date: string, salary: string, job_title: string, region_id: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing lists of structure /contents\n",
    "emp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,StringType,true),StructField(last_name,StringType,true),StructField(email,StringType,true),StructField(gender,StringType,true),StructField(department,StringType,true),StructField(start_date,StringType,true),StructField(salary,StringType,true),StructField(job_title,StringType,true),StructField(region_id,StringType,true)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing schema\n",
    "emp_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- region_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# more readable fortmat of schema\n",
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'last_name',\n",
       " 'email',\n",
       " 'gender',\n",
       " 'department',\n",
       " 'start_date',\n",
       " 'salary',\n",
       " 'job_title',\n",
       " 'region_id']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing columns\n",
    "emp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='1', last_name=\"'Kelley'\", email=\"'rkelley0@soundcloud.com'\", gender=\"'Female'\", department=\"'Computers'\", start_date=\"'10/2/2009'\", salary='67470', job_title=\"'Structural Engineer'\", region_id='2'),\n",
       " Row(id='2', last_name=\"'Armstrong'\", email=\"'sarmstrong1@infoseek.co.jp'\", gender=\"'Male'\", department=\"'Sports'\", start_date=\"'3/31/2008'\", salary='71869', job_title=\"'Financial Advisor'\", region_id='2'),\n",
       " Row(id='3', last_name=\"'Carr'\", email=\"'fcarr2@woothemes.com'\", gender=\"'Male'\", department=\"'Automotive'\", start_date=\"'7/12/2009'\", salary='101768', job_title=\"'Recruiting Manager'\", region_id='3'),\n",
       " Row(id='4', last_name=\"'Murray'\", email=\"'jmurray3@gov.uk'\", gender=\"'Female'\", department=\"'Jewelery'\", start_date=\"'12/25/2014'\", salary='96897', job_title=\"'Desktop Support Technician'\", region_id='3'),\n",
       " Row(id='5', last_name=\"'Ellis'\", email=\"'jellis4@sciencedirect.com'\", gender=\"'Female'\", department=\"'Grocery'\", start_date=\"'9/19/2002'\", salary='63702', job_title=\"'Software Engineer III'\", region_id='7')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the first five employees listed\n",
    "emp_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count function\n",
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with large dataset\n",
    "- use sampling (approximation)\n",
    "- use filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sample dataframe based on emp_df (e.g 10 %)\n",
    "sample_df = emp_df.sample(False, 0.1)\n",
    "# False paramenter because: for not replace sampling without replacement\n",
    "# 0.1 is 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.count()\n",
    "# the output of sampling is approximate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering (for manager whose salary is more than 100000)\n",
    "emp_mng_df = emp_df.filter('salary >= 100000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_mng_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|101768|\n",
      "|118497|\n",
      "|108657|\n",
      "|108093|\n",
      "|121966|\n",
      "|141139|\n",
      "|106659|\n",
      "|148952|\n",
      "|109890|\n",
      "|115274|\n",
      "|144724|\n",
      "|126103|\n",
      "|144965|\n",
      "|113507|\n",
      "|120579|\n",
      "|107222|\n",
      "|125668|\n",
      "|113857|\n",
      "|108378|\n",
      "|133424|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first 20 salaries in the filtered dataframe\n",
    "emp_mng_df.select('salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLlib packages\n",
    "- three types of functions\n",
    "1. Machine learning algorithms: algorithms for\n",
    "    - Classification : categorizing something (e.g customer likely to leave for a competitor)\n",
    "    - Regresion : predicting numeric value like a home price\n",
    "    - Clustering : to group similar items together (unlike classfication, no predifined groups, so useful for exploring data)\n",
    "    - Topic modeling : to identify themes in a text\n",
    "        \n",
    "2. Workflows : organization of commonly used steps\n",
    "    - Pre-processing\n",
    "    - Feature transformations\n",
    "    - Pipelines\n",
    "    - Evaluations\n",
    "    - Hyperparameter tunning\n",
    "3. Utilites ( distributed math an statistics functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (2 types)\n",
    "    - numeric data\n",
    "    - text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric: Normalize\n",
    "- Maps data values from their original range to the range of 0-1\n",
    "- Avoids problems when some attributes have large ranges and others have small ranges\n",
    "    - Salaryof employer(large range)\n",
    "    - Years of employemnt (small range)\n",
    "    \n",
    "### Numeric: Standardize\n",
    "- Map data vlaues from their original range to -1 to 1\n",
    "- Also mean value of 0\n",
    "- Normanlly distributed with standard deviation of 1\n",
    "- Used when attributes have different scales and ML algorithms assume normal distribution\n",
    "\n",
    "\n",
    "### Numeric: Partitioning\n",
    "- Map data values from continuous values to buckets\n",
    "- Deciles and percentiles are example fo buckets\n",
    "- Useful to work with groups of values instead of a contiuous range of values\n",
    "\n",
    "\n",
    "### Text : Tokenizing\n",
    "- Map text from a single string to a set of tokens/words\n",
    "- Example\n",
    "    - 'This is a book'\n",
    "    - ['This', 'is', 'a', 'book']\n",
    "    \n",
    "\n",
    "### Text: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- Map text from a single, typically long string, to a vector indicating the frequency of each word in a text relative to a group of texts (corpus)\n",
    "- Widely used in text classification\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## lets do some practice of those concepts above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaizing data\n",
    "# We do this so that differences in the scale of different features\n",
    "# do not adversely affect our models (e.g salary and miles)\n",
    "# importing some packages\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors # linear albebra\n",
    "\n",
    "# creating a simple dataframe (each row will include an identifier and a list of values)\n",
    "features_df = spark.createDataFrame([\n",
    "                                (1, Vectors.dense([10.0, 10000.0, 1.0])),\n",
    "                                (2, Vectors.dense([20.0, 30000.0, 2.0])),\n",
    "                                (3, Vectors.dense([30.0, 40000.0, 3.0]))\n",
    "],['id', 'features'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scaler object\n",
    "feature_scaler = MinMaxScaler(inputCol ='features', outputCol ='scaled_features')#Transform inputCol to new outputCol\n",
    "\n",
    "# fit model to the data (using fit function)\n",
    "scaled_model = feature_scaler.fit(features_df)\n",
    "\n",
    "# apply the transformation to create new scaled featured set\n",
    "scaled_features_df = scaled_model.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), scaled_features=SparseVector(3, {}))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look of first row\n",
    "scaled_features_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|          features|     scaled_features|\n",
      "+------------------+--------------------+\n",
      "|[10.0,10000.0,1.0]|           (3,[],[])|\n",
      "|[20.0,30000.0,2.0]|[0.5,0.6666666666...|\n",
      "|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's compare the original data with scaled version\n",
    "# now select all features\n",
    "scaled_features_df.select('features', 'scaled_features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarzing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we may have dta with nearly bell shape or normally distributed or not exactly, we do standardization for all cases\n",
    "- Some ML algorithms e.g support vector machine and some linear models work better when all of the features hae a unit variance and a zero mean\n",
    "- whey we apply standardization, our data slightly shifted to shape to becomes more normalized or more like bell curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# We use same dataframe of earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scaler object\n",
    "feature_standard_scaler = StandardScaler(inputCol ='features', outputCol ='standard_features', withStd = True, withMean =True) # withmean 0 (true)\n",
    "\n",
    "# fiting data to the model\n",
    "standard_model = feature_standard_scaler.fit(features_df)\n",
    "\n",
    "# Transform the data to new standardize form\n",
    "standard_features_df = standard_model.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), standard_features=DenseVector([-1.0, -1.0911, -1.0]))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_features_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, we see that in addtion to id column and features, we have also new column named standard_features that contains a dense vector with standardized data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+\n",
      "| id|          features|   standard_features|\n",
      "+---+------------------+--------------------+\n",
      "|  1|[10.0,10000.0,1.0]|[-1.0,-1.09108945...|\n",
      "|  2|[20.0,30000.0,2.0]|[0.0,0.2182178902...|\n",
      "|  3|[30.0,40000.0,3.0]|[1.0,0.8728715609...|\n",
      "+---+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look all values in the df\n",
    "standard_features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing Continuous data into buckets/partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "# bucketizer allows us to group data based on boundaires\n",
    "# so now we provide list of boundaries and called it splits\n",
    "splits = [-float('inf'), -10.0, 0.0, 10.0, float('inf')]\n",
    "# at the lower end i put negative infinity e.g -float('inf'), then -infinity to -10 another bucket and so on \n",
    "# last bucket is anything upto +10 and + infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|  -800.0|\n",
      "|   -10.5|\n",
      "|    -1.8|\n",
      "|     0.0|\n",
      "|     9.2|\n",
      "|    90.5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define some data\n",
    "b_data = [(-800.0,), (-10.5,), (-1.8,), (0.0,), (9.2,), (90.5,)]\n",
    "b_df = spark.createDataFrame(b_data, ['features'])\n",
    "b_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bucktizer object\n",
    "bucketizer = Bucketizer(splits = splits, inputCol = 'features', outputCol ='b_features')\n",
    "\n",
    "# transform the data\n",
    "bucketed_df = bucketizer.transform(b_df)\n",
    "\n",
    "# we don't need to fit the data here, as buckterizer is simple, and splits is list of boundaries i want for each bucket, so no need to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|features|b_features|\n",
      "+--------+----------+\n",
      "|  -800.0|       0.0|\n",
      "|   -10.5|       0.0|\n",
      "|    -1.8|       1.0|\n",
      "|     0.0|       2.0|\n",
      "|     9.2|       2.0|\n",
      "|    90.5|       3.0|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we see that features have been assigned to different buckets according to their size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new data frame for sentences(with 3 rows)\n",
    "sentences_df = spark.createDataFrame([\n",
    "    (1, 'This is an introduciton to Spark MLlib'),\n",
    "    (2, 'MLlib includes libraies for classification and regression'),\n",
    "    (3, 'It also contains supporting tools for piplines')\n",
    "], ['id', 'sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|           sentences|\n",
      "+---+--------------------+\n",
      "|  1|This is an introd...|\n",
      "|  2|MLlib includes li...|\n",
      "|  3|It also contains ...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer object\n",
    "sent_token = Tokenizer(inputCol = 'sentences', outputCol ='words')\n",
    "\n",
    "# tranformation function of the Tokenizer\n",
    "sent_tokenized_df = sent_token.transform(sentences_df)\n",
    "\n",
    "# here also, no need to fit the data, as in tokenization we will split up strings into separate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|           sentences|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|This is an introd...|[this, is, an, in...|\n",
      "|  2|MLlib includes li...|[mllib, includes,...|\n",
      "|  3|It also contains ...|[it, also, contai...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's have a look of tokenized dataframe\n",
    "sent_tokenized_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we see, the words column contains list of words with tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when we tokenize, we get list of words\n",
    "- we count number of times, a particular word appears\n",
    "- we do counting for all of our documents in our corpus (collection of documents)\n",
    "- we also count up, how often a term appears accross all of the documents\n",
    "- So, we have got 2 sets of counts:\n",
    "        - count of number of times the word show up in a single document\n",
    "        - count of how often those words sow up across all of the documents\n",
    "- These two numbers give us TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's work now with hashing tf and idf\n",
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, sentences: string]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the df\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentences='This is an introduciton to Spark MLlib')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first sentence\n",
    "sentences_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentences='This is an introduciton to Spark MLlib', words=['this', 'is', 'an', 'introduciton', 'to', 'spark', 'mllib'])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating hashing term frequency object\n",
    "hasingTF = HashingTF(inputCol ='words', outputCol='rawFeatures',numFeatures = 20)\n",
    "\n",
    "# apply transform to our tokenized sentences\n",
    "sent_hfTF_df = hasingTF.transform(sent_tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentences='This is an introduciton to Spark MLlib', words=['this', 'is', 'an', 'introduciton', 'to', 'spark', 'mllib'], rawFeatures=SparseVector(20, {6: 1.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 2.0, 15: 1.0}))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_hfTF_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that new column is a vector and it maps each word to an indices,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the rawFeature vector values based on how often the words appear in the entire collection of sentences\n",
    "\n",
    "# create an IDF object\n",
    "idf = IDF(inputCol ='rawFeatures', outputCol='idf_features')\n",
    "\n",
    "# fitting into inverse documnet frequency model\n",
    "idf_model = idf.fit(sent_hfTF_df)\n",
    "\n",
    "# apply transformation function to creaet new dataframe\n",
    "tfidf_df = idf_model.transform(sent_hfTF_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentences='This is an introduciton to Spark MLlib', words=['this', 'is', 'an', 'introduciton', 'to', 'spark', 'mllib'], rawFeatures=SparseVector(20, {6: 1.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 2.0, 15: 1.0}), idf_features=SparseVector(20, {6: 0.2877, 8: 0.2877, 9: 0.6931, 10: 0.6931, 13: 1.3863, 15: 0.0}))]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so now we have the dataframe that contains tf and idf data, lets have look of first data\n",
    "tfidf_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we see that, new column which contains the idf features along with old features\n",
    "- these are measures of each word relative to how frequently they occur in the entire corpus. In our case our corpus is just three sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of preprocessing transformation\n",
    "\n",
    "- Numeric transformation\n",
    "     - MinMaxScaler (map attributes from 0-1)\n",
    "     - StandardScaler (map attributes from -1 to 1)\n",
    "     - Bucketizer (create partitions for grouping values)\n",
    "    \n",
    "- Text transformation\n",
    "     - Tokenizer (splitting string into list of words)\n",
    "     - HashingTF (creating TF-IDF vectors from text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (useful for data exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering algorithms\n",
    "- group data into clusters that allow us to see how large data sets can break down into distinct subgroups (looking for macro level structures)\n",
    "- K-means : finding cluster in small and mid-sized dataset\n",
    "- Hierarchical clustering with Bisecting K-means : for larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "cluster_df = spark.read.csv('.\\clustering_dataset.csv', header=True, inferSchema = True)\n",
    "# use infer schema as we want to work with numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col1: int, col2: int, col3: int]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   7|   4|   1|\n",
      "|   7|   7|   9|\n",
      "|   7|   9|   6|\n",
      "|   1|   6|   5|\n",
      "|   6|   7|   7|\n",
      "|   7|   9|   4|\n",
      "|   7|  10|   6|\n",
      "|   7|   8|   2|\n",
      "|   8|   3|   8|\n",
      "|   4|  10|   5|\n",
      "|   7|   4|   5|\n",
      "|   7|   8|   4|\n",
      "|   2|   5|   1|\n",
      "|   2|   6|   2|\n",
      "|   2|   3|   8|\n",
      "|   3|   9|   1|\n",
      "|   4|   2|   9|\n",
      "|   1|   7|   1|\n",
      "|   6|   2|   3|\n",
      "|   4|   1|   9|\n",
      "+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   7|   4|   1|\n",
      "|   7|   7|   9|\n",
      "|   7|   9|   6|\n",
      "|   1|   6|   5|\n",
      "|   6|   7|   7|\n",
      "|   7|   9|   4|\n",
      "|   7|  10|   6|\n",
      "|   7|   8|   2|\n",
      "|   8|   3|   8|\n",
      "|   4|  10|   5|\n",
      "|   7|   4|   5|\n",
      "|   7|   8|   4|\n",
      "|   2|   5|   1|\n",
      "|   2|   6|   2|\n",
      "|   2|   3|   8|\n",
      "|   3|   9|   1|\n",
      "|   4|   2|   9|\n",
      "|   1|   7|   1|\n",
      "|   6|   2|   3|\n",
      "|   4|   1|   9|\n",
      "|   4|   8|   5|\n",
      "|   6|   6|   7|\n",
      "|   4|   6|   2|\n",
      "|   8|   1|   1|\n",
      "|   7|   5|  10|\n",
      "|  17|  25|  21|\n",
      "|  15|  23|  32|\n",
      "|  42|  25|  45|\n",
      "|  41|  47|  21|\n",
      "|  37|  20|  27|\n",
      "|  40|  18|  26|\n",
      "|  41|  28|  50|\n",
      "|  32|  25|  40|\n",
      "|  24|  29|  35|\n",
      "|  47|  18|  47|\n",
      "|  36|  42|  45|\n",
      "|  49|  29|  15|\n",
      "|  47|  39|  22|\n",
      "|  38|  27|  25|\n",
      "|  45|  23|  40|\n",
      "|  23|  36|  19|\n",
      "|  47|  40|  50|\n",
      "|  37|  30|  40|\n",
      "|  42|  48|  41|\n",
      "|  29|  31|  21|\n",
      "|  36|  39|  48|\n",
      "|  50|  24|  31|\n",
      "|  42|  44|  37|\n",
      "|  37|  39|  46|\n",
      "|  22|  40|  30|\n",
      "|  17|  29|  41|\n",
      "|  85| 100|  69|\n",
      "|  68|  76|  67|\n",
      "|  76|  70|  93|\n",
      "|  62|  66|  91|\n",
      "|  83|  93|  76|\n",
      "|  95|  72|  63|\n",
      "|  75|  94|  95|\n",
      "|  83|  72|  80|\n",
      "|  93|  87|  76|\n",
      "|  86|  93|  63|\n",
      "|  97|  82|  75|\n",
      "|  61|  74|  74|\n",
      "|  84|  90| 100|\n",
      "|  77|  67|  97|\n",
      "|  61|  82|  73|\n",
      "|  81|  60|  69|\n",
      "|  67|  80|  98|\n",
      "|  94|  82|  60|\n",
      "|  69|  73|  74|\n",
      "|  74|  96|  80|\n",
      "|  86|  62|  61|\n",
      "|  88|  68|  95|\n",
      "|  99|  67|  80|\n",
      "|  76|  95|  70|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df.show(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we see that the data is grouped into 3 clusters. e.g first 25 rows with value between 0-10, second 25 rows between  15-60 and so on\n",
    "# so the data is natually clustered\n",
    "# we do now litte transformation to put these columns into a feature vector (single vector column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector assembler\n",
    "vectorAssembler = VectorAssembler(inputCols= ['col1', 'col2', 'col3'], outputCol = 'features')\n",
    "\n",
    "# transformation\n",
    "vector_cluster_df = vectorAssembler.transform(cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------------+\n",
      "|col1|col2|col3|      features|\n",
      "+----+----+----+--------------+\n",
      "|   7|   4|   1| [7.0,4.0,1.0]|\n",
      "|   7|   7|   9| [7.0,7.0,9.0]|\n",
      "|   7|   9|   6| [7.0,9.0,6.0]|\n",
      "|   1|   6|   5| [1.0,6.0,5.0]|\n",
      "|   6|   7|   7| [6.0,7.0,7.0]|\n",
      "|   7|   9|   4| [7.0,9.0,4.0]|\n",
      "|   7|  10|   6|[7.0,10.0,6.0]|\n",
      "|   7|   8|   2| [7.0,8.0,2.0]|\n",
      "|   8|   3|   8| [8.0,3.0,8.0]|\n",
      "|   4|  10|   5|[4.0,10.0,5.0]|\n",
      "|   7|   4|   5| [7.0,4.0,5.0]|\n",
      "|   7|   8|   4| [7.0,8.0,4.0]|\n",
      "|   2|   5|   1| [2.0,5.0,1.0]|\n",
      "|   2|   6|   2| [2.0,6.0,2.0]|\n",
      "|   2|   3|   8| [2.0,3.0,8.0]|\n",
      "|   3|   9|   1| [3.0,9.0,1.0]|\n",
      "|   4|   2|   9| [4.0,2.0,9.0]|\n",
      "|   1|   7|   1| [1.0,7.0,1.0]|\n",
      "|   6|   2|   3| [6.0,2.0,3.0]|\n",
      "|   4|   1|   9| [4.0,1.0,9.0]|\n",
      "+----+----+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_cluster_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we see the differences, we have added up one new column which is a single feature vector\n",
    "# we did this because K-means algorithm will work with that features vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an obect Kmeans\n",
    "kmeans = KMeans().setK(3) # se the number of cluster or K to 3\n",
    "\n",
    "# set the C which deternmines where the KMeans algorithm starts,\n",
    "# it is useful if we are doing testing, to have consistency\n",
    "kmeans = kmeans.setSeed(1)\n",
    "\n",
    "# fit our data into model\n",
    "k_model = kmeans.fit(vector_cluster_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([35.88461538, 31.46153846, 34.42307692]),\n",
       " array([80.        , 79.20833333, 78.29166667]),\n",
       " array([5.12, 5.84, 4.84])]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the centers of these clusters\n",
    "centers = k_model.clusterCenters()\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we see Kmeans discover three clusters and they have centererd, for example one cluster is centered around the points 35, 31, 34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-------------+\n",
      "|col1|col2|col3|     features|\n",
      "+----+----+----+-------------+\n",
      "|   7|   4|   1|[7.0,4.0,1.0]|\n",
      "|   7|   7|   9|[7.0,7.0,9.0]|\n",
      "|   7|   9|   6|[7.0,9.0,6.0]|\n",
      "|   1|   6|   5|[1.0,6.0,5.0]|\n",
      "|   6|   7|   7|[6.0,7.0,7.0]|\n",
      "+----+----+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_cluster_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This df contains 3 columns with raw data\n",
    "- a fourth column, which is features, which maps those 3 columns into a vector reprsentation, which is used by k-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will apply bisecting k-means on that\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "# creating bisecting k-means object\n",
    "bkmeans = BisectingKMeans().setK(3)\n",
    "bkmeans = bkmeans.setSeed(1)\n",
    "\n",
    "# create a model\n",
    "bk_model = bkmeans.fit(vector_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5.12, 5.84, 4.84]),\n",
       " array([35.88461538, 31.46153846, 34.42307692]),\n",
       " array([80.        , 79.20833333, 78.29166667])]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating bkcenters object\n",
    "bkcenters = bk_model.clusterCenters()\n",
    "bkcenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([35.88461538, 31.46153846, 34.42307692]),\n",
       " array([80.        , 79.20833333, 78.29166667]),\n",
       " array([5.12, 5.84, 4.84])]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall the k-means centers\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we see they are same or pretty close \n",
    "# Keep in mind that different algorithms may find different centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- splitting data into different categories (category A or category B)\n",
    "- we will see 3 classfication algorithms here e.g\n",
    "        - Naive Bayes\n",
    "        - Decision trees\n",
    "        - Multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the Iris data set\n",
    "from pyspark.sql.functions import * # pyspark sql functionality\n",
    "from pyspark.ml.feature import VectorAssembler # preprocessing tools\n",
    "from pyspark.ml.feature import StringIndexer # preprocessing tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading iris data\n",
    "iris_df = spark.read.csv('.\\iris1.csv',inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=5.1, _c1=3.5, _c2=1.4, _c3=0.2, _c4='Iris-setosa')]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that the columns name are just co0, c1 and so on. so we can rename these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----------+\n",
      "|_c0|_c1|_c2|_c3|        _c4|\n",
      "+---+---+---+---+-----------+\n",
      "|5.1|3.5|1.4|0.2|Iris-setosa|\n",
      "|4.9|3.0|1.4|0.2|Iris-setosa|\n",
      "|4.7|3.2|1.3|0.2|Iris-setosa|\n",
      "+---+---+---+---+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appply sql functionality to improve the dataframe by renaming the column name\n",
    "iris_df = iris_df.select(col('_c0').alias('sepal_length'),\n",
    "                        col('_c1').alias('sepal_width'),\n",
    "                        col('_c2').alias('petal_length'),\n",
    "                        col('_c3').alias('petal_width'),\n",
    "                        col('_c4').alias('species')\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='Iris-setosa')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at first row\n",
    "iris_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|    species|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's transform this into vector structure\n",
    "# create vector assembler\n",
    "vectorAssembler = VectorAssembler(inputCols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol = 'features')\n",
    "\n",
    "# apply transformation\n",
    "v_iris_df = vectorAssembler.transform(iris_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='Iris-setosa', features=DenseVector([5.1, 3.5, 1.4, 0.2]))]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_iris_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we see that row with 4 measurements and new column named features with the vector of the four measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the label name (species) into numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the transformation called StringIndexer \n",
    "# create an indexer\n",
    "indexer = StringIndexer(inputCol ='species', outputCol='label')\n",
    "\n",
    "# create dataframe that capture this indexed value (fitting data)\n",
    "index_iris_df = indexer.fit(v_iris_df).transform(v_iris_df) # in shortcut apply also transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|    species|         features|label|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the first row\n",
    "index_iris_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that we have all 5 old columns and also new column named label\n",
    "# we see that label has 0, as this species (iris-setosa) has been mapped to an index value of 0\n",
    "# so we are ready with classfication algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string, features: vector, label: double]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we have the dataframe which has the indexed and vectorized iris data\n",
    "\n",
    "# recalling the data\n",
    "# structue of data\n",
    "index_iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='Iris-setosa', features=DenseVector([5.1, 3.5, 1.4, 0.2]), label=0.0)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_iris_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see our raw data (first 5 columns from the file),one feature vector (some numeric measures) and  one label (0 in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes # naivebayes classifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # to evaluate the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's split our data\n",
    "splits = index_iris_df.randomSplit([0.6, 0.4], 1) # call randomSplit method\n",
    "# one dataframe will return 60% of data and the other with 40 % of data, and 1 for the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- The random split will return a list, in this case it will have two dataframe\n",
    "- First one is train dataframe and can be accessed by index 0 and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting iris data set into train and test data set\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 52, 150)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the lenght of train, test and original dataframe\n",
    "train_df.count(), test_df.count(), index_iris_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create naive bayes classifier\n",
    "nb = NaiveBayes(modelType='multinomial') # we used multinomial as we have 3 iris classes\n",
    "\n",
    "# fit our training data to a model\n",
    "nb_model = nb.fit(train_df)\n",
    "\n",
    "# so, now we have a model that we can make predictions with\n",
    "# Creating prediction data frame\n",
    "predictions_df = nb_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=4.3, sepal_width=3.0, petal_length=1.1, petal_width=0.1, species='Iris-setosa', features=DenseVector([4.3, 3.0, 1.1, 0.1]), label=0.0, rawPrediction=DenseVector([-9.9894, -11.3476, -11.902]), probability=DenseVector([0.7118, 0.183, 0.1051]), prediction=0.0)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at first row of predictions data frame\n",
    "predictions_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the prediction is correct, but looking at one example does not tell us how well the model behaves overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some through evaluation\n",
    "# Create an evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol ='label', predictionCol ='prediction', metricName = 'accuracy')\n",
    "# Here we specify our label column, which in this case is simply label, and we're going to compare our label to what was predicted. Our prediction column is called \"prediction,\" and the metric that we're trying to measure is accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results and call the evaluator and evaluate our predictions\n",
    "nb_accuracy = evaluator.evaluate(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9807692307692307"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we see the accuracy is pretty good with NaiveBayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at other classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron classification \n",
    "- it is a type of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string, features: vector, label: double]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use the same dataset- the indexed and vectorised version of dataframe\n",
    "index_iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='Iris-setosa', features=DenseVector([5.1, 3.5, 1.4, 0.2]), label=0.0)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the data structure\n",
    "index_iris_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here,features are feature vector which take 4 measuremnts and put them into a single vector\n",
    "# label map the string species name into a number 0, 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importig modules to support multilayer perceptron\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Now, the way a multi-layer perceptron classifier works is that we have, as the name implies, multiple levels of neurons. Now in all cases, the first layer has the same number of nodes as there are inputs. So for us we have four measures so our first layer will be four. So I'm going to create a list of layers. I'm going to set the first element to be four. Now the last element should have the same number of neurons as there are types of outputs. Now in our case there's three types of iris species. So our last row will be three. Now we want to have layers in between, and the layers in between will help the multi-layer perceptron learn how to classify correctly. So I'm going to insert two rows of five neurons each. So we are going to have a four layer multi-layer perceptron. First layer will have four neurons, the middle two layers will have five neurons each, and then the output layer will have three neurons. One for each kind of iris species.\\n\""
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Now, the way a multi-layer perceptron classifier works is that we have, as the name implies, multiple levels of neurons. Now in all cases, the first layer has the same number of nodes as there are inputs. So for us we have four measures so our first layer will be four. So I'm going to create a list of layers. I'm going to set the first element to be four. Now the last element should have the same number of neurons as there are types of outputs. Now in our case there's three types of iris species. So our last row will be three. Now we want to have layers in between, and the layers in between will help the multi-layer perceptron learn how to classify correctly. So I'm going to insert two rows of five neurons each. So we are going to have a four layer multi-layer perceptron. First layer will have four neurons, the middle two layers will have five neurons each, and then the output layer will have three neurons. One for each kind of iris species.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of layers\n",
    "layers = [4,5,5,3] # input layer (4), middle two layers are 5 neuron each, output layer (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multilayer perceptron\n",
    "mlp = MultilayerPerceptronClassifier(layers = layers, seed= 1)\n",
    "\n",
    "# build a model and fit on train data\n",
    "mlp_model = mlp.fit(train_df)\n",
    "\n",
    "# create predction and transform on test data\n",
    "mlp_predictions = mlp_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the predictions\n",
    "# create an evaluator\n",
    "mlp_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy\n",
    "mlp_accuracy = mlp_evaluator.evaluate(mlp_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy is not goot compared to naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index_iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='Iris-setosa', features=DenseVector([5.1, 3.5, 1.4, 0.2]), label=0.0)]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_iris_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree object (classifier)\n",
    "dt = DecisionTreeClassifier(labelCol='label', featuresCol='features')\n",
    "\n",
    "# Create a model and fit on train data\n",
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "# create prediction (using the model) and transform on test data\n",
    "dt_predictions = dt_model.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### steps\n",
    "- So I've created a classifier, which I call dt. \n",
    "- Using dt, I built a model by fitting our training data to that \n",
    "- and then I used the model and applied the transform over our test data to make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, evaluate those predictions\n",
    "# create an evaluator \n",
    "dt_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "\n",
    "# measure the accuracy\n",
    "dt_accuracy= dt_evaluator.evaluate(dt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9423076923076923"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so the accuracy is quite good with DecisionTree classifier\n",
    "- we observed that NaiveBayes and DecisionTree quite works well with our dataset\n",
    "- But the MultilayerPerceptron did not work well with iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "- it is helpful to experiment with a number of different algorithms and number of different configurations, if that's required by the algorithm\n",
    "- Naive Bayes work well- if attributes in the dataset are independent to each other (didn't corelate tightly)\n",
    "- Multilayer perceptron - works well with non-linear data\n",
    "- DecisionTree - good option in many cases and it is worth starting with decision trees and then try other algorithms from there.                                                                                    \n",
    "                                                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "- Allow us to make predictions about numeric values\n",
    "- to make projections into the future\n",
    "\n",
    "- We wil focus here 3 regression algorighms\n",
    "        - Linear Regression\n",
    "        - Decision Tree Regression\n",
    "        - Gradient Boost Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statment: To predict how much power a plant can generate, based on some factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import codes for linear regression\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data into dataframe\n",
    "# first i read the data in pandas dataframe (for excel file)\n",
    "import pandas\n",
    "df = pandas.read_excel('./power_plant.xlsx', sheet_name='Sheet1',inferSchema=True)\n",
    "\n",
    "# open the data in spark session \n",
    "pp_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AT: double, V: double, AP: double, RH: double, PE: double]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at data structure\n",
    "pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double means numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AT', 'V', 'AP', 'RH', 'PE']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create a feature vector \n",
    "# create vectorAssembler object\n",
    "vectorAssembler = VectorAssembler(inputCols= ['AT', 'V', 'AP', 'RH'], outputCol='features')\n",
    "\n",
    "# create new dataframe and apply transform \n",
    "vector_pp_df = vectorAssembler.transform(pp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AT=14.96, V=41.76, AP=1024.07, RH=73.17, PE=463.26, features=DenseVector([14.96, 41.76, 1024.07, 73.17]))]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at one row\n",
    "vector_pp_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we are ready to create linear regression model, after creating the feature vector\n",
    "# Now, create linear regression object\n",
    "lr = LinearRegression(featuresCol='features', labelCol='PE')\n",
    "\n",
    "# Build the model by fitting data (fit the model)\n",
    "lr_model = lr.fit(vector_pp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-1.9775, -0.2339, 0.0621, -0.1581])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at some features of the model e.g\n",
    "lr_model.coefficients \n",
    "# it gives us 4 numbers which correspond to the coefficents of differnet variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454.6092742751131"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.intercept\n",
    "# it gives us a point where the line crosses the Y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So basically we fit a line to our data (that's what linear regression is all about)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.557126016749482"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# error: quality of the linear model\n",
    "lr_model.summary.rootMeanSquaredError\n",
    "# it is a measure of how much error there is in our predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So we're looking at an error of about 4.56 on a value in the 455 range is the intercept, \n",
    "- so in terms of scale we have an error of around 1% which is actually quite good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "lr_model.save('lr1.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor # regression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator # evaluator\n",
    "from pyspark.ml.feature import VectorAssembler # vector feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+\n",
      "|   AT|    V|     AP|   RH|    PE|\n",
      "+-----+-----+-------+-----+------+\n",
      "|14.96|41.76|1024.07|73.17|463.26|\n",
      "|25.18|62.96|1020.04|59.08|444.37|\n",
      "| 5.11| 39.4|1012.16|92.14|488.56|\n",
      "+-----+-----+-------+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we will use the same data set of power plant\n",
    "pp_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorAssembler object\n",
    "vectorAssembler = VectorAssembler(inputCols= ['AT', 'V', 'AP', 'RH'], outputCol='features')\n",
    "\n",
    "# create new dataframe and apply transform \n",
    "vector_pp_df = vectorAssembler.transform(pp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AT=14.96, V=41.76, AP=1024.07, RH=73.17, PE=463.26, features=DenseVector([14.96, 41.76, 1024.07, 73.17]))]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_pp_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train and test data\n",
    "splits = vector_pp_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# create train data frame\n",
    "train_df = splits[0]\n",
    "\n",
    "#create test data frame\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6628, 2940, 9568)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count(), test_df.count(), vector_pp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a decision tree object\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol='PE')\n",
    "\n",
    "# create a model and fit training data to it\n",
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "# make predictions and apply transform to test data\n",
    "dt_predictions = dt_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "- So we've created our decision tree regressor, \n",
    "- we fed our training data to it \n",
    "- and we created some predictions by applying our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our results\n",
    "dt_evaluator = RegressionEvaluator(labelCol='PE', predictionCol='prediction', metricName='rmse')\n",
    "# On classification model, we used accuracy \n",
    "# Here, as we will do numeric prediction, we ar more interested in how far off our predictions are, so we will use error metric (Rmse)\n",
    "\n",
    "# calculate the error\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.575757924726427"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we see that the result is pretty good in terms of rmse and very close to the error of linearn regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# create instance of the regressor\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='PE')\n",
    "\n",
    "# create the model and fit the training data\n",
    "gbt_model = gbt.fit(train_df)\n",
    "\n",
    "# create prediction and apply transform to the test data\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "\n",
    "# create a (gbt)evaluator (to evaluate the model)\n",
    "gbt_evaluator = RegressionEvaluator(labelCol='PE', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "# getting rmse from gbt model\n",
    "gbt_rmse =gbt_evaluator.evaluate(gbt_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1252760455724955"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is slightly better than other two regression methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conlusions\n",
    "- Regression algorithms are designed to make numeric projections\n",
    "- Experiment with multiple algorithms and \n",
    "- See which works best for the particular data set\n",
    "- Because it is easy to experiment as you just need to add several lines of code\n",
    "\n",
    "\n",
    "    - Linear regression is simple and widely used\n",
    "    - Decision tree works as does lineear regression\n",
    "    - Gradient-boost sometimes give best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand recommendation systme\n",
    "# Spark MLlib collaborative filtering\n",
    "# we can use Alternating least square (ALS) method for that\n",
    "# import ALS from pyspark.ml.recommendation\n",
    "\n",
    "# all the process is same as above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "#### Three basic stages of building machine learning model\n",
    "- preprocessing : collect, reform and transform\n",
    "        - load data into DataFrame\n",
    "        - include headers, or column names, in text files\n",
    "        - use inferSchema=True (will make sure dates and numeric values get mapped to their appropriate data type)\n",
    "        - use VectorAssembler to create feature vectors\n",
    "        - use StringIndexer to map from string to numeric indexes\n",
    "- model building: apply machine learning algorithms to training data\n",
    "        - split data into trianing and test sets\n",
    "        - fit models using trianing data\n",
    "        - create predictions by applying trnasform to the test data\n",
    "- validation: assess the quality of models built in step 2\n",
    "        - use MLlib evaluators\n",
    "            - MulticlassClassificationEvaluator\n",
    "            - RegressionEvaluator\n",
    "        - Experiment with multiple algorithms (e.g lr, dt, gbt)\n",
    "        - vary hyperparameters for the algorithms working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS public data set for big data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
